\Section{Introduction}{}

\subsection{Basic Notions}

\begin{definition}
    For a field \( \F \), an \( \F \)-vector space is an collection \( (V, +,
    \cdot) \) with \( + : V \times V \to V \) and \( \cdot : \F \times V \to V
    \) satisfying the following axioms:
    \begin{enumerate}
        \item \( +, \cdot \) are associative.
        \item \( + \) is commutative.
        \item There exists some \( \vec{0} \in V \) which is an additive
            identity.
        \item Additive inverses exist.
        \item \( 1_\F \cdot \vec{v} = \vec{v} \) for all \( \vec{v} \in V \).
        \item Distributivity of \( +, \cdot \).
    \end{enumerate}
\end{definition}

\begin{theorem}
    Let \( \vec{v} \in V \). Then, \( 0 \vec{v} = \vec{0} \).
\end{theorem}

\begin{proof}
    Observe that
    \begin{align*}
        0 \vec{v} &= (0 + 0) \vec{v} \\
        &= 0\vec{v} + 0 \vec{v}
    .\end{align*}
    Adding the additive inverse on both sides yields the claim.
\end{proof}

\begin{theorem}
    Let \( \vec{v} \in V \). Then, \( (-1) \cdot \vec{v} = -\vec{v} \) (the
    additive inverse of \( \vec{v} \)).
\end{theorem}

\begin{proof}
    By the previous theorem, we have that \( 0 \cdot \vec{v} = \vec{0} \). Thus,
    \[
        (1 - 1) \cdot \vec{v} = 0
    ,\]
    and since \( 1 \cdot \vec{v} = \vec{v} \), our claim follows.
\end{proof}

\begin{theorem}
    Let \( c \in \F, \vec{v} \in V \) be such that \( c \vec{v} = \vec{0} \). Then, either \( c = 0 \) or \( \vec{v} = 0 \).
\end{theorem}

\begin{proof}
    Observe that if \( c \ne 0 \), then we may write \( \vec{0} = c \cdot (0
    \cdot \vec{v}) \), so we have that \( \vec{v} = 0 \vec{v} = \vec{0} \) via
    multiplying by \( c^{-1} \).
\end{proof}

\begin{definition}
    A list of vectors of length \( k \) is a sequence of \( k \) vectors \(
    (\vec{v}_1, \ldots, \vec{v}_k) \).
\end{definition}

Note that lists are taken to be finite in this class. Lists also may be empty.

\begin{definition}
    Given a list \( \mathcal{L} = (\vec{v}_1, \ldots, \vec{v}_n) \) of vectors, a linear combination of \( \mathcal{L} \) is a sum
    \[
        \sum_{i = 1}^{n} \alpha_i \vec{v}_i
    ,\]
    for \( \alpha_1, \ldots, \alpha_n \in \F \).
\end{definition}

\begin{definition}
    The span of a list \( \mathcal{L} \), denoted by \( \Span(\mathcal{L}) \),
    is the set of vectors \( \vec{v} \in V \) that can be represented as a
    linear combination of \( \mathcal{L} \).
\end{definition}

\begin{definition}
    A list \( \mathcal{L} \) is independent if each vector in \( \Span(\mathcal{L}) \) has a unique representation as a linear combination of \( \mathcal{L} \).
\end{definition}

\subsection{Utilities}

\begin{theorem}
    \( \mathcal{L} = (\vec{v}_1, \ldots, \vec{v}_n) \) is independent iff \( \vec{0} \) has only the trivial
    representation.
\end{theorem}

\begin{proof}
    The forward direction is trivial (all vectors must have unique
    representation). We shall prove the backward direction: \( \mathcal{L} \)
    dependent implies \( \vec{0} \) has nontrivial representation. Indeed, since \( \mathcal{L} \) is dependent, there exists some vector \( \vec{v} \) such that
    \[
        \vec{v} = \sum_{i = 1}^{n} \alpha_i \vec{v}_i = \sum_{i = 1}^{n} \beta_i \vec{v}_i
    ,\]
    where \( \alpha_j \ne \beta_j \) for some \( j \). Subtracting,
    \[
        \vec{0} = \sum_{i = 1}^{n} (\alpha_i - \beta_i) \vec{v}_i
    .\]
    Since \( \alpha_j - \beta_j \ne 0 \), \( \vec{0} \) has a nontrivial representation.
\end{proof}

\begin{theorem}
    If \( \mathcal{L} = (\vec{v}_1, \ldots, \vec{v}_n) \) is dependent, then
    there exists some \( j \in \{1, \ldots, n\} \) such that
    \[
        \vec{v}_j = \sum_{i = 1}^{j - 1} \alpha_i \vec{v}_i  
    .\]
\end{theorem}

\begin{proof}
    Since \( \mathcal{L} \) is dependent, there exists some nontrivial representation of \( \vec{0} \) given by
    \[
        \vec{0} = \sum_{i = 1}^{n} \alpha_i \vec{v}_i 
    .\]
    Select \( j \) to be the maximal value such that \( \alpha_i \ne 0 \) so that
    \[
        \vec{0} = \alpha_j \vec{v}_j + \sum_{i = 1}^{j - 1} \alpha_i \vec{v}_i
    .\]
    Then, we have that
    \[
        \vec{v}_j = -\sum_{i = 1}^{j - 1} \frac{\alpha_i}{\alpha_j}  \vec{v}_i
    ,\]
    as desired.
\end{proof}

\begin{theorem}
    Let \( \mathcal{L} \) be a list in \( V \). Then, \( \mathcal{L} \)
    contains a linearly independent sublist with the same span.
\end{theorem}

\begin{proof}
    We may assume that \( \mathcal{L} \) is dependent (otherwise trivial). Let
    \( \mathcal{L}' \) be a smallest sublist whose span is \(
    \Span(\mathcal{L}) \) (well-ordering principle). We claim that \(
    \mathcal{L}' \) is independent.

    BWOC suppose \( \mathcal{L}' = (\vec{v}_1, \ldots, \vec{v}_k) \) is
    dependent. Then, by the previous theorem, there is some \( j \) for which
    \( \vec{v}_j \) is a linear combination of \( \vec{v}_1, \ldots,
    \vec{v}_{j-1} \). Thus, the list \( (\vec{v}_1, \ldots, \vec{v}_{j-1},
    \vec{v}_{j+1}, \ldots, \vec{v}_n) \) has the same span and is strictly
    smaller in size, which contradicts that \( \mathcal{L}' \) was of minimal
    size. Thus, \( \mathcal{L}' \) is independent.
\end{proof}

\begin{lemma}
    Suppose \( \mathcal{L} = (\vec{v}_1, \ldots, \vec{v}_n) \) and \(
    \vec{w}_1, \ldots, \vec{w}_k \in \Span(\mathcal{L}) \). Then, \(
    \Span(\vec{w}_1, \ldots, \vec{w}_k) \subseteq \Span(\mathcal{L}) \).
\end{lemma}

\begin{proof}
    Write each \( \vec{w}_i \) as a linear combination of \( \mathcal{L} \).
\end{proof}

\begin{definition}
    A list \( \mathcal{B} \) is a basis for \( V \) if every \( \vec{v} \in V
    \) can be written uniquely as a linear combination of \( \mathcal{B} \).
    Equivalently, \( \mathcal{B} \) is a basis of \( V \) iff \(
    \Span(\mathcal{B}) = V \) and \( \mathcal{B} \) is independent.
\end{definition}
